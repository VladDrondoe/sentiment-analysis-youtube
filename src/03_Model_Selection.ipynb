{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6685ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from model_selection_functions import train_models, prepare_train_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6db9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/youtube_comments_preprocessed_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bada13c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommentText       0\n",
       "Sentiment         0\n",
       "Likes             0\n",
       "Comment_Length    0\n",
       "Month             0\n",
       "DayOfWeek         0\n",
       "Hour              0\n",
       "IsWeekend         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6831194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fb0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.sample(50000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4cfb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c3d2333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommentText       0\n",
       "Sentiment         0\n",
       "Likes             0\n",
       "Replies           0\n",
       "Comment_Length    0\n",
       "Month             0\n",
       "DayOfWeek         0\n",
       "Hour              0\n",
       "IsWeekend         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614ad4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, tfidf = prepare_train_test(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3135d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64e0d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    solver=\"saga\",\n",
    "    penalty=\"l2\",\n",
    "    C=2.0,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "svc = LinearSVC(\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=50,\n",
    "    objective=\"multiclass\",\n",
    "    num_class=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "list_of_models = [lr, svc, lgbm, xgb,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16a3a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== LogisticRegression =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   : 0.3815\n",
      "Macro F1   : 0.3354\n",
      "Confusion matrix:\n",
      "[[1263  112 2030]\n",
      " [ 851  276 2145]\n",
      " [ 822  225 2276]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.37      0.40      3405\n",
      "           1       0.45      0.08      0.14      3272\n",
      "           2       0.35      0.68      0.47      3323\n",
      "\n",
      "    accuracy                           0.38     10000\n",
      "   macro avg       0.41      0.38      0.34     10000\n",
      "weighted avg       0.41      0.38      0.34     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "===== LinearSVC =====\n",
      "Accuracy   : 0.6431\n",
      "Macro F1   : 0.6430\n",
      "Confusion matrix:\n",
      "[[2208  778  419]\n",
      " [ 789 1929  554]\n",
      " [ 448  581 2294]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.64      3405\n",
      "           1       0.59      0.59      0.59      3272\n",
      "           2       0.70      0.69      0.70      3323\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.64      0.64      0.64     10000\n",
      "weighted avg       0.64      0.64      0.64     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "===== LGBMClassifier =====\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 132602\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 2357\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "Accuracy   : 0.6828\n",
      "Macro F1   : 0.6830\n",
      "Confusion matrix:\n",
      "[[2325  662  418]\n",
      " [ 587 2193  492]\n",
      " [ 420  593 2310]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69      3405\n",
      "           1       0.64      0.67      0.65      3272\n",
      "           2       0.72      0.70      0.71      3323\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.68      0.68      0.68     10000\n",
      "weighted avg       0.68      0.68      0.68     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "===== XGBClassifier =====\n",
      "Accuracy   : 0.6751\n",
      "Macro F1   : 0.6760\n",
      "Confusion matrix:\n",
      "[[2263  776  366]\n",
      " [ 530 2296  446]\n",
      " [ 396  735 2192]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.69      3405\n",
      "           1       0.60      0.70      0.65      3272\n",
      "           2       0.73      0.66      0.69      3323\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.68      0.68      0.68     10000\n",
      "weighted avg       0.68      0.68      0.68     10000\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best model: LGBMClassifier (Accuracy = 0.6828, Macro F1 = 0.6830)\n"
     ]
    }
   ],
   "source": [
    "best_models = train_models(list_of_models, X_train, X_test,y_train ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "830be85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small2=df_small.copy()\n",
    "df_small2[\"Likes\"] = np.log1p(df[\"Likes\"])\n",
    "df_small2[\"Replies\"] = np.log1p(df[\"Replies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2fd2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2, tfidf2 = prepare_train_test(df_small2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e278589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    max_iter=8000,\n",
    "    C=2.0,\n",
    "    class_weight=\"balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "871c35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.55      0.56      3405\n",
      "           1       0.55      0.48      0.51      3272\n",
      "           2       0.57      0.67      0.62      3323\n",
      "\n",
      "    accuracy                           0.57     10000\n",
      "   macro avg       0.57      0.57      0.57     10000\n",
      "weighted avg       0.57      0.57      0.57     10000\n",
      "\n",
      "Accuracy: 0.5688\n",
      "F1 Score: 0.5661226719642591\n",
      "Confusion Matrix:\n",
      " [[1870  726  809]\n",
      " [ 838 1575  859]\n",
      " [ 514  566 2243]]\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train2, y_train2)\n",
    "y_pred =  lr.predict(X_test)\n",
    "print(classification_report(y_test2, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test2, y_pred, average='weighted'))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d369bc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65      3405\n",
      "           1       0.59      0.60      0.59      3272\n",
      "           2       0.72      0.69      0.71      3323\n",
      "\n",
      "    accuracy                           0.65     10000\n",
      "   macro avg       0.65      0.65      0.65     10000\n",
      "weighted avg       0.65      0.65      0.65     10000\n",
      "\n",
      "Accuracy: 0.6501\n",
      "F1 Score: 0.6507267045731183\n",
      "Confusion Matrix:\n",
      " [[2241  803  361]\n",
      " [ 788 1965  519]\n",
      " [ 448  580 2295]]\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_train2, y_train2)\n",
    "y_pred = svc.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test2, y_pred, average='weighted'))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test2, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a48b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    n_estimators=1500 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2620e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"learning_rate\": [0.02, 0.03, 0.05, 0.08, 0.1],\n",
    "    \"num_leaves\": [31, 63, 127],\n",
    "    \"min_data_in_leaf\": [20, 30, 50, 80, 120],\n",
    "    \"feature_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"bagging_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"bagging_freq\": [0, 5, 10],\n",
    "    \"lambda_l2\": [0.0, 0.5, 1.0, 2.0, 5.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8339225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173743\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 5985\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n",
      "Best Macro F1: 0.6731740289148461\n",
      "Best Params: {'num_leaves': 63, 'min_data_in_leaf': 20, 'learning_rate': 0.02, 'lambda_l2': 5.0, 'feature_fraction': 0.7, 'bagging_freq': 10, 'bagging_fraction': 0.6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=25, \n",
    "    scoring=\"f1_macro\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Macro F1:\", search.best_score_)\n",
    "print(\"Best Params:\", search.best_params_)\n",
    "best_lgbm = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f5d69cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] lambda_l2 is set=5.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.6906568998178252\n",
      "Accuracy: 0.6905\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_lgbm.predict(X_test)\n",
    "print(\"Macro F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa74cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2c2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection_functions import prepare_train_test_delete_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c04b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train3, y_test3, tfidf3 = prepare_train_test_delete_stop_words(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7be15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    n_estimators=1500,\n",
    "    #best params from RandomizedSearch\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=20,\n",
    "    learning_rate=0.02,\n",
    "    reg_lambda=5.0,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_freq=10,\n",
    "    bagging_fraction=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce992b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 83606\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 3094\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.6750780022787247\n",
      "Accuracy: 0.6744\n"
     ]
    }
   ],
   "source": [
    "best_lgbm.fit(X_train3, y_train3)\n",
    "y_pred = best_lgbm.predict(X_test3)\n",
    "print(\"Macro F1:\", f1_score(y_test3, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8380dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2302  730  373]\n",
      " [ 553 2183  536]\n",
      " [ 363  701 2259]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test3, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f126ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "0    17023\n",
       "2    16614\n",
       "1    16363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35d25f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import lemmatize_spacy_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7f70a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma=df_small.copy()\n",
    "df_lemma['CommentText'] = lemmatize_spacy_series(df_lemma['CommentText'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c544900",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemma, X_test_lemma, y_train_lemma, y_test_lemma, tfidf_lemma = prepare_train_test_delete_stop_words(df_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bcd0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm_lemma = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    n_estimators=1500,\n",
    "    #best params from RandomizedSearch\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=20,\n",
    "    learning_rate=0.02,\n",
    "    reg_lambda=5.0,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_freq=10,\n",
    "    bagging_fraction=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab5c733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 79981\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 2770\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.6721684296283601\n",
      "Accuracy: 0.6716\n",
      "[[2296  734  375]\n",
      " [ 554 2176  542]\n",
      " [ 398  681 2244]]\n"
     ]
    }
   ],
   "source": [
    "best_lgbm_lemma.fit(X_train_lemma, y_train_lemma)\n",
    "y_pred = best_lgbm_lemma.predict(X_test_lemma)\n",
    "print(\"Macro F1:\", f1_score(y_test_lemma, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test_lemma, y_pred))\n",
    "print(confusion_matrix(y_test_lemma, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e0c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection_functions import prepare_train_test_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a63937",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4, X_test4, y_train4, y_test4, tfidf4 = prepare_train_test_v2(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6956d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.020596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1290402\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 23452\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.6991476078522595\n",
      "Accuracy: 0.6992\n",
      "[[2433  613  359]\n",
      " [ 555 2191  526]\n",
      " [ 386  569 2368]]\n"
     ]
    }
   ],
   "source": [
    "best_lgbm.fit(X_train4, y_train4)\n",
    "y_pred = best_lgbm.predict(X_test4)\n",
    "print(\"Macro F1:\", f1_score(y_test4, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test4, y_pred))\n",
    "print(confusion_matrix(y_test4, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef42dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection_functions import prepare_train_test_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d6b2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4, X_test4, y_train4, y_test4, tfidf4 = prepare_train_test_v3(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a246da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.195608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1290427\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 23452\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.6978756653182788\n",
      "Accuracy: 0.6979\n",
      "[[2426  620  359]\n",
      " [ 558 2188  526]\n",
      " [ 385  573 2365]]\n"
     ]
    }
   ],
   "source": [
    "best_lgbm.fit(X_train4, y_train4)\n",
    "y_pred = best_lgbm.predict(X_test4)\n",
    "print(\"Macro F1:\", f1_score(y_test4, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test4, y_pred))\n",
    "print(confusion_matrix(y_test4, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7441db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection_functions import prepare_train_test_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d45a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train5, X_test5, y_train5, y_test5, tfidf5 = prepare_train_test_only_text(df_small)\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    max_iter=8000,\n",
    "    C=2.0,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "svc = LinearSVC(\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6cab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67      3405\n",
      "           1       0.60      0.62      0.61      3272\n",
      "           2       0.73      0.68      0.70      3323\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.66      0.66      0.66     10000\n",
      "weighted avg       0.66      0.66      0.66     10000\n",
      "\n",
      "Accuracy: 0.6577\n",
      "F1 Score: 0.658101118548118\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train5, y_train5)\n",
    "y_pred =  lr.predict(X_test5)\n",
    "print(classification_report(y_test5, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test5, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test5, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf5ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65      3405\n",
      "           1       0.59      0.59      0.59      3272\n",
      "           2       0.71      0.69      0.70      3323\n",
      "\n",
      "    accuracy                           0.65     10000\n",
      "   macro avg       0.65      0.64      0.64     10000\n",
      "weighted avg       0.65      0.65      0.65     10000\n",
      "\n",
      "Accuracy: 0.645\n",
      "F1 Score: 0.6448429536459587\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_train5, y_train5)\n",
    "y_pred = svc.predict(X_test5)\n",
    "print(classification_report(y_test5, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test5, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test5, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf00af",
   "metadata": {},
   "source": [
    "Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd54025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24cc5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_small[\"Sentiment\"]\n",
    "df_train, df_test, y_train, y_test = train_test_split(df_small, y, test_size=0.2, stratify=y, random_state=42)\n",
    "numeric_features = [\"Likes\", \"Replies\", \"Comment_Length\", \"Month\", \"DayOfWeek\", \"Hour\", \"IsWeekend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c44e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 676\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "          feature  importance_mean  importance_std\n",
      "0           Month         0.066638        0.001661\n",
      "1           Likes         0.056785        0.004278\n",
      "2       DayOfWeek         0.055044        0.003073\n",
      "3  Comment_Length         0.034385        0.001756\n",
      "4            Hour         0.020038        0.001780\n",
      "5         Replies         0.009483        0.001611\n",
      "6       IsWeekend         0.000277        0.001092\n"
     ]
    }
   ],
   "source": [
    "X_other_train = df_train[numeric_features].astype(float)\n",
    "X_other_test  = df_test[numeric_features].astype(float)\n",
    "\n",
    "best_lgbm.fit(X_other_train, y_train)\n",
    "\n",
    "f1_macro = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "perm = permutation_importance(\n",
    "    best_lgbm,\n",
    "    X_other_test,\n",
    "    y_test,\n",
    "    scoring=f1_macro,\n",
    "    n_repeats=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": numeric_features,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(imp_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d8115",
   "metadata": {},
   "source": [
    "Trying Keras for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5aa3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "from model_selection_functions import prepare_text_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ef9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, vectorizer = prepare_text_datasets(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27f7934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m5,120,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m82,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,202,435</span> (19.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,202,435\u001b[0m (19.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,202,435</span> (19.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,202,435\u001b[0m (19.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorizer(inputs)\n",
    "x = layers.Embedding(40000, 128)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd68320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 63ms/step - accuracy: 0.5442 - loss: 0.9447 - val_accuracy: 0.6451 - val_loss: 0.8145\n",
      "Epoch 2/6\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.7014 - loss: 0.7065 - val_accuracy: 0.6474 - val_loss: 0.8122\n",
      "Epoch 3/6\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.7907 - loss: 0.5294 - val_accuracy: 0.6320 - val_loss: 0.8988\n",
      "Epoch 4/6\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.8630 - loss: 0.3732 - val_accuracy: 0.6144 - val_loss: 1.0388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1838aa4d550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
    "\n",
    "model.fit(train_ds,validation_data=test_ds,epochs=6,callbacks=callbacks)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd755d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Accuracy: 0.6474\n",
      "Macro F1: 0.6462212320354186\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2183  751  471]\n",
      " [ 765 1888  619]\n",
      " [ 437  483 2403]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Macro F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d18eca",
   "metadata": {},
   "source": [
    "Trying two_channel for lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8771697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "from model_selection_functions import prepare_train_test_two_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6f2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, (tfidf_word, tfidf_char) = prepare_train_test_two_channel(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0bd95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.200898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1372328\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 26503\n",
      "[LightGBM] [Info] Start training from score -1.077487\n",
      "[LightGBM] [Info] Start training from score -1.116954\n",
      "[LightGBM] [Info] Start training from score -1.101792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.695601707801284\n",
      "Accuracy: 0.6957\n",
      "[[2427  610  368]\n",
      " [ 564 2175  533]\n",
      " [ 393  575 2355]]\n"
     ]
    }
   ],
   "source": [
    "best_lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_lgbm.predict(X_test)\n",
    "\n",
    "print(\"Macro F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ec600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Replies'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51461ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selection_functions import prepare_train_test_v3_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d3e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4, X_test4, y_train4, y_test4, tfidf4 = prepare_train_test_v3_for_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7a7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"multiclass\",\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    n_estimators=5000,\n",
    "    n_jobs=-1,\n",
    "\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=20,\n",
    "    learning_rate=0.02,\n",
    "    reg_lambda=5.0,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_freq=10,\n",
    "    bagging_fraction=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f4cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 49.690841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3708180\n",
      "[LightGBM] [Info] Number of data points in the train set: 795083, number of used features: 50000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Info] Start training from score -1.082869\n",
      "[LightGBM] [Info] Start training from score -1.110904\n",
      "[LightGBM] [Info] Start training from score -1.102270\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\tvalid_0's multi_logloss: 0.580502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\sda\\Proiecte\\sentiment-analysis-youtube\\venv\\Lib\\site-packages\\lightgbm\\basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "Macro F1: 0.7508330271610036\n",
      "Accuracy: 0.7507131321973527\n",
      "[[51568 10516  5224]\n",
      " [ 9205 47683  8560]\n",
      " [ 6428  9618 49969]]\n"
     ]
    }
   ],
   "source": [
    "best_lgbm.fit(X_train4, y_train4,eval_set=[(X_test4, y_test4)],eval_metric=\"multi_logloss\",callbacks=[lgb.early_stopping(50, verbose=True)])\n",
    "y_pred = best_lgbm.predict(X_test4,num_iteration=best_lgbm.best_iteration_)\n",
    "print(\"Macro F1:\", f1_score(y_test4, y_pred, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test4, y_pred))\n",
    "print(confusion_matrix(y_test4, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7df22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d41c7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Models/sentiment_bundle_lgbm.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bundle = {\n",
    "    \"model\": best_lgbm,\n",
    "    \"tfidf\": tfidf4,\n",
    "}\n",
    "\n",
    "joblib.dump(bundle, \"../Models/sentiment_bundle_lgbm.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
